{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4. Listen Attend Spell","provenance":[{"file_id":"1nlDSJGRW7jgo-wmCsn0WHAn8G8y1WqD2","timestamp":1659056444091}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5acbd787bf724b298c2d289744ef66fd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a2f2f506ed9045a2864978e81df4062e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_191b17a34c394310b00c989ff0690b53","IPY_MODEL_76708ab4783d4882b5c33ba6d135a4a9"]},"model_module_version":"1.5.0"},"a2f2f506ed9045a2864978e81df4062e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"191b17a34c394310b00c989ff0690b53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cc2f39e4e5574207bb8285edead9c592","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":6387309499,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":6387309499,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c83f4f53ebd443ed965ed438b27d34b0"},"model_module_version":"1.5.0"},"76708ab4783d4882b5c33ba6d135a4a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_92c7bb2f44d44e1bb3f9c9fc45054a8f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5.95G/5.95G [04:06&lt;00:00, 26.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_32ab6de4c1e04e258b20545704b7264f"},"model_module_version":"1.5.0"},"cc2f39e4e5574207bb8285edead9c592":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"c83f4f53ebd443ed965ed438b27d34b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"92c7bb2f44d44e1bb3f9c9fc45054a8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"32ab6de4c1e04e258b20545704b7264f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"29215b7037b3458188b8809282fd72df":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2689d7f2a46146f09976aeaaa2ea9a95","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0f0171610d604eb0b5b92194d502eda2","IPY_MODEL_95430f18cd6348b7a448bfcfc0d9812b"]},"model_module_version":"1.5.0"},"2689d7f2a46146f09976aeaaa2ea9a95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"0f0171610d604eb0b5b92194d502eda2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b6dc3c88cdc64697a94d18689f3abfa4","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":346663984,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":346663984,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_597039f064d84288a812954117494cfa"},"model_module_version":"1.5.0"},"95430f18cd6348b7a448bfcfc0d9812b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e476fc39f93d45bab331e59310dfc890","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 331M/331M [00:13&lt;00:00, 26.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4a5f508fd1434f57b93641a33df6c704"},"model_module_version":"1.5.0"},"b6dc3c88cdc64697a94d18689f3abfa4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"597039f064d84288a812954117494cfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"e476fc39f93d45bab331e59310dfc890":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"4a5f508fd1434f57b93641a33df6c704":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"}}}},"cells":[{"cell_type":"code","metadata":{"id":"hbFBOE8uwJ7V","colab":{"base_uri":"https://localhost:8080/","height":222},"executionInfo":{"status":"ok","timestamp":1590714009109,"user_tz":-540,"elapsed":11454,"user":{"displayName":"도승헌","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64","userId":"15552087445877711483"}},"outputId":"5525fe40-3c62-448a-b0b1-7e2af1c1a838"},"source":["!pip install torch\n","!pip install torchaudio"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n","Collecting torchaudio\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/7d/8e01e21175dd2c9bb1b7e014e0c56cdd02618e2db5bebb4f52f6fdf253cb/torchaudio-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 2.6MB/s \n","\u001b[?25hRequirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.5.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->torchaudio) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->torchaudio) (1.18.4)\n","Installing collected packages: torchaudio\n","Successfully installed torchaudio-0.5.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3TqGPsmzKiUY"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"6iN54BqnSTpk"},"source":["\n","### Reference \n","\n","- Digital Signal Processing Lecture\u000bhttps://github.com/spatialaudio/digital-signal-processing-lecture \n","\n","- Python for Signal Processing (unipingco)\u000bhttps://github.com/unpingco/Python-for-Signal-Processing \n","\n","- Audio for Deep Learning (남기현님)\u000bhttps://tykimos.github.io/2019/07/04/ISS_2nd_Deep_Learning_Conference_All_Together/ \n","\n","- 오디오 전처리 작업을 위한 연습 (박수철님)\u000bhttps://github.com/scpark20/audio-preprocessing-practice \n","\n","- Musical Applications of Machine Learning\u000bhttps://mac.kaist.ac.kr/~juhan/gct634/ \n","\n","- Awesome audio study materials for Korean (최근우님)\u000bhttps://github.com/keunwoochoi/awesome-audio-study-materials-for-korean"]},{"cell_type":"code","metadata":{"id":"F4hfYNb2wu9m"},"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchaudio\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bb06mH8jxGtr","colab":{"base_uri":"https://localhost:8080/","height":114,"referenced_widgets":["5acbd787bf724b298c2d289744ef66fd","a2f2f506ed9045a2864978e81df4062e","191b17a34c394310b00c989ff0690b53","76708ab4783d4882b5c33ba6d135a4a9","cc2f39e4e5574207bb8285edead9c592","c83f4f53ebd443ed965ed438b27d34b0","92c7bb2f44d44e1bb3f9c9fc45054a8f","32ab6de4c1e04e258b20545704b7264f","29215b7037b3458188b8809282fd72df","2689d7f2a46146f09976aeaaa2ea9a95","0f0171610d604eb0b5b92194d502eda2","95430f18cd6348b7a448bfcfc0d9812b","b6dc3c88cdc64697a94d18689f3abfa4","597039f064d84288a812954117494cfa","e476fc39f93d45bab331e59310dfc890","4a5f508fd1434f57b93641a33df6c704"]},"executionInfo":{"status":"ok","timestamp":1590708945160,"user_tz":-540,"elapsed":372010,"user":{"displayName":"도승헌","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64","userId":"15552087445877711483"}},"outputId":"c4a5849c-d25d-4e9b-9cab-a2a988472771"},"source":["train_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True) \n","test_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"test-clean\", download=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5acbd787bf724b298c2d289744ef66fd","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=6387309499.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29215b7037b3458188b8809282fd72df","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=346663984.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N8JvgqogxgtA","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1590709005863,"user_tz":-540,"elapsed":3248,"user":{"displayName":"도승헌","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64","userId":"15552087445877711483"}},"outputId":"4191e052-48f7-4797-a26e-932c4a0bccf9"},"source":["test_dataset[1]\n","#소리 데이터, 샘플레이트 "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0.0007, 0.0008, 0.0007,  ..., 0.0015, 0.0010, 0.0015]]),\n"," 16000,\n"," 'AT THE INCEPTION OF PLURAL MARRIAGE AMONG THE LATTER DAY SAINTS THERE WAS NO LAW NATIONAL OR STATE AGAINST ITS PRACTISE',\n"," 4077,\n"," 13754,\n"," 9)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"wlBQJ2QopRFk"},"source":["def avg_wer(wer_scores, combined_ref_len):\n","    return float(sum(wer_scores)) / float(combined_ref_len)\n","\n","\n","def _levenshtein_distance(ref, hyp):\n","    \"\"\"\"Levenshtein distance\"는 두 시퀀스 간의 차이를 측정하기위한 문자열 메트릭입니다. \n","    \"Levenshtein distanc\"는 한 단어를 다른 단어로 변경하는 데 필요한 최소 한 문자 편집 (대체, 삽입 또는 삭제) 수로 정의됩니다. \n","    \"\"\"\n","    m = len(ref)\n","    n = len(hyp)\n","\n","    # special case\n","    if ref == hyp:\n","        return 0\n","    if m == 0:\n","        return n\n","    if n == 0:\n","        return m\n","\n","    if m < n:\n","        ref, hyp = hyp, ref\n","        m, n = n, m\n","\n","    # use O(min(m, n)) space\n","    distance = np.zeros((2, n + 1), dtype=np.int32)\n","\n","    # initialize distance matrix\n","    for j in range(0,n + 1):\n","        distance[0][j] = j\n","\n","    # calculate levenshtein distance\n","    for i in range(1, m + 1):\n","        prev_row_idx = (i - 1) % 2\n","        cur_row_idx = i % 2\n","        distance[cur_row_idx][0] = i\n","        for j in range(1, n + 1):\n","            if ref[i - 1] == hyp[j - 1]:\n","                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n","            else:\n","                s_num = distance[prev_row_idx][j - 1] + 1\n","                i_num = distance[cur_row_idx][j - 1] + 1\n","                d_num = distance[prev_row_idx][j] + 1\n","                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n","\n","    return distance[m % 2][n]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ru55tshPqejt"},"source":["\n","def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n","    \"\"\"참조 시퀀스와 가설 시퀀스 사이의 거리를 단어 수준으로 계산합니다.\n","     : param reference : 참조 문장.\n","     : param hypothesis : 가설 문장.\n","     : param ignore_case : 대소 문자 구분 여부.\n","     : param delimiter : 입력 문장의 구분자.\n","    \"\"\"\n","    if ignore_case == True:\n","        reference = reference.lower()\n","        hypothesis = hypothesis.lower()\n","\n","    ref_words = reference.split(delimiter)\n","    hyp_words = hypothesis.split(delimiter)\n","\n","    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n","    return float(edit_distance), len(ref_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d-LCRpXVqhv8"},"source":["def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n","    if ignore_case == True:\n","        reference = reference.lower()\n","        hypothesis = hypothesis.lower()\n","\n","    join_char = ' '\n","    if remove_space == True:\n","        join_char = ''\n","\n","    reference = join_char.join(filter(None, reference.split(' ')))\n","    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n","\n","    edit_distance = _levenshtein_distance(reference, hypothesis)\n","    return float(edit_distance), len(reference)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gBZ_3-Sqm5D"},"source":["def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n","    \"\"\"Calculate word error rate (WER). \n","    WER = (Sw + Dw + Iw) / Nw\n","    Sw는 대체 된 단어의 수입니다.\n","    Dw는 삭제 된 단어의 수입니다.\n","    Iw는 삽입 된 단어의 수입니다.\n","    Nw는 참조의 단어 수입니다.\n","    \"\"\"\n","    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n","                                         delimiter)\n","    if ref_len == 0:\n","        raise ValueError(\"Reference's word number should be greater than 0.\")\n","\n","    wer = float(edit_distance) / ref_len\n","    return wer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F6YUEk-2qqaw"},"source":["def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n","    \"\"\"Calculate charactor error rate (CER). \n","        CER = (Sc + Dc + Ic) / Nc\n","        Sc is the number of characters substituted,\n","        Dc is the number of characters deleted,\n","        Ic is the number of characters inserted\n","        Nc is the number of characters in the reference\n","    \"\"\"\n","    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n","                                         remove_space)\n","\n","    if ref_len == 0:\n","        raise ValueError(\"Length of reference should be greater than 0.\")\n","\n","    cer = float(edit_distance) / ref_len\n","    return cer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cS2jhIrBqs_G"},"source":["class TextTransform:\n","    \"\"\"Maps characters to integers and vice versa\"\"\"\n","    def __init__(self):\n","        char_map_str = \"\"\"\n","        ' 0\n","        <SPACE> 1\n","        a 2\n","        b 3\n","        c 4\n","        d 5\n","        e 6\n","        f 7\n","        g 8\n","        h 9\n","        i 10\n","        j 11\n","        k 12\n","        l 13\n","        m 14\n","        n 15\n","        o 16\n","        p 17\n","        q 18\n","        r 19\n","        s 20\n","        t 21\n","        u 22\n","        v 23\n","        w 24\n","        x 25\n","        y 26\n","        z 27\n","        \"\"\"\n","        self.char_map = {}\n","        self.index_map = {}\n","        for line in char_map_str.strip().split('\\n'):\n","            ch, index = line.split()\n","            self.char_map[ch] = int(index)\n","            self.index_map[int(index)] = ch\n","        self.index_map[1] = ' '\n","\n","    def text_to_int(self, text):\n","        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n","        int_sequence = []\n","        for c in text:\n","            if c == ' ':\n","                ch = self.char_map['<SPACE>']\n","            else:\n","                ch = self.char_map[c]\n","            int_sequence.append(ch)\n","        return int_sequence\n","\n","    def int_to_text(self, labels):\n","        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n","        string = []\n","        for i in labels:\n","            string.append(self.index_map[i])\n","        return ''.join(string).replace('<SPACE>', ' ')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HMUWPlL8qwQg"},"source":["train_audio_transforms = nn.Sequential(\n","    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n","    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n","    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",")\n","\n","valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n","\n","text_transform = TextTransform()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VrbecqsFq8qM"},"source":["train_audio_transforms = nn.Sequential(\n","    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n","    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n","    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",")\n","\n","valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n","\n","text_transform = TextTransform()\n","\n","def data_processing(data, data_type=\"train\"):\n","    spectrograms = []\n","    labels = []\n","    input_lengths = []\n","    label_lengths = []\n","    for (waveform, _, utterance, _, _, _) in data:\n","        if data_type == 'train':\n","            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        elif data_type == 'valid':\n","            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        else:\n","            raise Exception('data_type should be train or valid')\n","        spectrograms.append(spec)\n","        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n","        labels.append(label)\n","        input_lengths.append(spec.shape[0]//2)\n","        label_lengths.append(len(label))\n","\n","    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n","    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n","\n","    return spectrograms, labels, input_lengths, label_lengths\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXAYnQ4arD2P"},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, n_layers=1,\n","                 input_dropout_p=0, dropout_p=0,\n","                 bidirectional=False, rnn_cell='gru', variable_lengths=False):\n","\n","        self.hidden_size = hidden_size\n","        self.bidirectional = bidirectional\n","        self.n_layers = n_layers\n","        self.dropout_p = dropout_p\n","        self.variable_lengths = variable_lengths\n","\n","        if rnn_cell.lower() == 'lstm':\n","            self.rnn_cell = nn.LSTM\n","        elif rnn_cell.lower() == 'gru':\n","            self.rnn_cell = nn.GRU\n","        else:\n","            raise ValueError(\"Unsupported RNN Cell: {0}\".format(rnn_cell))\n","\n","        \"\"\"\n","        Copied from https://github.com/SeanNaren/deepspeech.pytorch/blob/master/model.py\n","        Copyright (c) 2017 Sean Naren\n","        MIT License\n","        \"\"\"\n","        outputs_channel = 32\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, outputs_channel, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n","            nn.BatchNorm2d(outputs_channel),\n","            nn.Hardtanh(0, 20, inplace=True), # relu의 범위를 0-20으로 한다.\n","            nn.Conv2d(outputs_channel, outputs_channel, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n","            nn.BatchNorm2d(outputs_channel),\n","            nn.Hardtanh(0, 20, inplace=True)\n","        )\n","        # Conv로 feature를 뽑고 relu의 범위를 0-20으로 하는 것을 Hardtanh로 구현\n","\n","        rnn_input_dims = int(math.floor(input_size + 2 * 20 - 41) / 2 + 1)\n","        rnn_input_dims = int(math.floor(rnn_input_dims + 2 * 10 - 21) / 2 + 1)\n","        rnn_input_dims *= outputs_channel\n","        \n","        self.rnn =  self.rnn_cell(rnn_input_dims, self.hidden_size, self.n_layers, dropout=self.dropout_p, bidirectional=self.bidirectional)\n","        # rnn layer를 쌓음\n","\n","    def forward(self, input_var, input_lengths=None):\n","        \"\"\"\n","        param:input_var: Encoder inputs, Spectrogram, Shape=(B,1,D,T)\n","        param:input_lengths: inputs sequence length without zero-pad\n","        \"\"\"\n","        output_lengths = self.get_seq_lens(input_lengths)\n","\n","        x = input_var # (B,1,D,T)\n","        x, _ = self.conv(x, output_lengths) # (B, C, D, T)\n","        \n","        x_size = x.size()\n","        x = x.view(x_size[0], x_size[1] * x_size[2], x_size[3]) # (B, C * D, T)\n","        x = x.transpose(1, 2).transpose(0, 1).contiguous() # (T, B, D)\n","\n","        x = nn.utils.rnn.pack_padded_sequence(x, output_lengths)\n","        x, h_state = self.rnn(x)\n","        x, _ = nn.utils.rnn.pad_packed_sequence(x)\n","        x = x.transpose(0, 1) # (B, T, D)\n","        return x, h_state\n","\n","\n","    def get_seq_lens(self, input_length):\n","        seq_len = input_length\n","        for m in self.conv.modules():\n","            if type(m) == nn.modules.conv.Conv2d :\n","                seq_len = ((seq_len + 2 * m.padding[1] - m.dilation[1] * (m.kernel_size[1] - 1) - 1) / m.stride[1] + 1)\n","\n","        return seq_len.int()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMITaH3ALmr8"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Attention(nn.Module):\n","    def __init__(self, dec_dim, enc_dim, conv_dim, attn_dim):\n","        super(Attention, self).__init__()\n","        self.dec_dim = dec_dim\n","        self.enc_dim = enc_dim\n","        self.conv_dim = conv_dim\n","        self.attn_dim = attn_dim\n","        self.conv = nn.Conv1d(in_channels=1, out_channels=self.attn_dim, kernel_size=3, padding=1)\n","\n","        self.W = nn.Linear(self.dec_dim, self.attn_dim, bias=False)\n","        self.V = nn.Linear(self.enc_dim, self.attn_dim, bias=False)\n","\n","        self.fc = nn.Linear(attn_dim, 1, bias=True)\n","        self.b = nn.Parameter(torch.rand(attn_dim))\n","\n","        self.tanh = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, queries, values, last_attn):\n","        \"\"\"\n","        param:quries: Decoder hidden states, Shape=(B,1,dec_D)\n","        param:values: Encoder outputs, Shape=(B,enc_T,enc_D)\n","        param:last_attn: Attention weight of previous step, Shape=(batch, enc_T)\n","        \"\"\"\n","        batch_size = queries.size(0)\n","        dec_feat_dim = queries.size(2)\n","        enc_feat_len = values.size(1)\n","\n","        # conv_attn = (B, enc_T, conv_D)\n","        conv_attn = torch.transpose(self.conv(last_attn.unsqueeze(dim=1)), 1, 2)\n","\n","        # (B, enc_T)\n","        # attention score를 정하는 부분\n","        # query와 value를 더하고 conv를 통해 나온 값과 bias를 더해 attention score를 만듦\n","        # 이때 fc를 쓰는데 안에 tanh를 사용한다.\n","        score =  self.fc(self.tanh(\n","         self.W(queries) + self.V(values) + conv_attn + self.b\n","        )).squeeze(dim=-1)\n","\n","        attn_weight = self.softmax(score) \n","        # (B, 1, enc_T) * (B, enc_T, enc_D) -> (B, 1, enc_D) \n","        context = torch.bmm(attn_weight.unsqueeze(dim=1), values)\n","        return context, attn_weight"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C0wPUMERrnjX"},"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, vocab_size, max_len, hidden_size, encoder_size,\n","                 sos_id, eos_id,\n","                 n_layers=1, rnn_cell='gru', \n","                 bidirectional_encoder=False, bidirectional_decoder=False,\n","                 dropout_p=0, use_attention=True):\n","\n","        self.output_size = vocab_size\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.bidirectional_encoder = bidirectional_encoder\n","        self.bidirectional_decoder = bidirectional_decoder\n","        self.encoder_output_size = encoder_size * 2 if self.bidirectional_encoder else encoder_size\n","        self.n_layers = n_layers\n","        self.dropout_p = dropout_p\n","        self.max_length = max_len\n","        self.use_attention = use_attention\n","        self.eos_id = eos_id\n","        self.sos_id = sos_id\n","        \n","        if rnn_cell.lower() == 'lstm':\n","            self.rnn_cell = nn.LSTM\n","        elif rnn_cell.lower() == 'gru':\n","            self.rnn_cell = nn.GRU\n","        else:\n","            raise ValueError(\"Unsupported RNN Cell: {0}\".format(rnn_cell))\n","\n","        self.init_input = None\n","        self.rnn = self.rnn_cell(self.hidden_size + self.encoder_output_size, self.hidden_size, self.n_layers,\n","                                 batch_first=True, dropout=dropout_p, bidirectional=self.bidirectional_decoder)\n","\n","        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n","        self.attention = Attention(dec_dim=self.hidden_size, enc_dim=self.encoder_output_size, conv_dim=1, attn_dim=self.hidden_size)\n","        self.fc = nn.Linear(self.hidden_size + self.encoder_output_size, self.output_size)\n","\n","    def forward_step(self, input_var, hidden, encoder_outputs, context, attn_w, function):\n","        if self.training:\n","            self.rnn.flatten_parameters()\n","\n","        batch_size = input_var.size(0)\n","        dec_len = input_var.size(1)\n","        enc_len = encoder_outputs.size(1)\n","        enc_dim = encoder_outputs.size(2)\n","        embedded = self.embedding(input_var) # (B, dec_T, voc_D) -> (B, dec_T, dec_D)\n","        embedded = self.input_dropout(embedded)\n","\n","        y_all = []\n","        attn_w_all = []\n","        for i in range(embedded.size(1)):\n","            embedded_inputs = embedded[:, i, :] # (B, dec_D)\n","            \n","            rnn_input = torch.cat([embedded_inputs, context], dim=1) # (B, dec_D + enc_D)\n","            rnn_input = rnn_input.unsqueeze(1) \n","            output, hidden = self.rnn(rnn_input, hidden) # (B, 1, dec_D)\n","\n","            context, attn_w = self.attention(output, encoder_outputs, attn_w) # (B, 1, enc_D), (B, enc_T)\n","            attn_w_all.append(attn_w)\n","            \n","            context = context.squeeze(1)\n","            output = output.squeeze(1) # (B, 1, dec_D) -> (B, dec_D)\n","            context = self.input_dropout(context)\n","            output = self.input_dropout(output)\n","            output = torch.cat((output, context), dim=1) # (B, dec_D + enc_D)\n","\n","            pred = function(self.fc(output), dim=-1)\n","            y_all.append(pred)\n","\n","        return y_all, hidden, context, attn_w_all\n","\n","\n","    def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None,\n","                    function=F.log_softmax):\n","        \"\"\"\n","        param:inputs: Decoder inputs sequence, Shape=(B, dec_T)\n","        param:encoder_hidden: Encoder last hidden states, Default : None\n","        param:encoder_outputs: Encoder outputs, Shape=(B,enc_T,enc_D)\n","        \"\"\"\n","    \n","        batch_size = encoder_outputs.size(0)\n","        inputs = torch.LongTensor([self.sos_id] * batch_size).view(batch_size, 1)\n","        max_length = self.max_length\n","\n","        decoder_hidden = None\n","        context = encoder_outputs.new_zeros(batch_size, encoder_outputs.size(2)) # (B, D)\n","        attn_w = encoder_outputs.new_zeros(batch_size, encoder_outputs.size(1)) # (B, T)\n","        # 앞서 만들었던 attention weight를 활용하는 것을 볼 수 있다. \n","        decoder_outputs = []\n","        sequence_symbols = []\n","        lengths = np.array([max_length] * batch_size)\n","\n","        def decode(step, step_output):\n","            decoder_outputs.append(step_output)\n","            symbols = decoder_outputs[-1].topk(1)[1]\n","            sequence_symbols.append(symbols)\n","            eos_batches = symbols.data.eq(self.eos_id)\n","            if eos_batches.dim() > 0:\n","                eos_batches = eos_batches.cpu().view(-1).numpy()\n","                update_idx = ((lengths > step) & eos_batches) != 0\n","                lengths[update_idx] = len(sequence_symbols)\n","            return symbols\n","\n","        decoder_input = inputs[:, 0].unsqueeze(1)\n","        for di in range(max_length):\n","            decoder_output, decoder_hidden, context, attn_w = self.forward_step(decoder_input, \n","                                                                                decoder_hidden,\n","                                                                                encoder_outputs,\n","                                                                                context,\n","                                                                                attn_w,\n","                                                                                function=function)\n","            step_output = decoder_output.squeeze(1)\n","            symbols = decode(di, step_output)\n","            decoder_input = symbols\n","\n","        return decoder_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gyojINGV98zz"},"source":[""],"execution_count":null,"outputs":[]}]}