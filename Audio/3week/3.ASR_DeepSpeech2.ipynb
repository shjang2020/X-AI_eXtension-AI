{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.ASR_DeepSpeech2","provenance":[{"file_id":"1Ttk_urX_dUvimaqlrhglC6ql3XNKKqSS","timestamp":1659056432104}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4e1b1bce5c6f46fca0d63d3c5a93f9d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c4a5238581ae454cbac49ef583e81c47","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c07bf3c2e2bf4ec7a794902d3d21d882","IPY_MODEL_24e332e807774ac9b476dc4dd1f714a6"]},"model_module_version":"1.5.0"},"c4a5238581ae454cbac49ef583e81c47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"c07bf3c2e2bf4ec7a794902d3d21d882":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_178f1c9b1dfa4652ab4dc9494e2cc9af","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":6387309499,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":6387309499,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c3f09958981c45f58d25171c0e9eab3c"},"model_module_version":"1.5.0"},"24e332e807774ac9b476dc4dd1f714a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_53f193912acc46d0800ee862cb645e30","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5.95G/5.95G [09:25&lt;00:00, 11.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8b67b1196f9f4c3aaccb8c7c6bd1e3d0"},"model_module_version":"1.5.0"},"178f1c9b1dfa4652ab4dc9494e2cc9af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"c3f09958981c45f58d25171c0e9eab3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"53f193912acc46d0800ee862cb645e30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"8b67b1196f9f4c3aaccb8c7c6bd1e3d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"26a74eca32854fc5930a5d3ad68b5e9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_afb6f5fcb4064bdb99623edb90743bd2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_619ca67e4591441e91494bbf86b51216","IPY_MODEL_4c7b3f5773134e4388b3aeee523b3027"]},"model_module_version":"1.5.0"},"afb6f5fcb4064bdb99623edb90743bd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"619ca67e4591441e91494bbf86b51216":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f7482ec2de31467dbea4753d71b8294d","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":346663984,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":346663984,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c6b0848093294970a297138cd9552e03"},"model_module_version":"1.5.0"},"4c7b3f5773134e4388b3aeee523b3027":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2f6e069899f9470880ad18de95dcc539","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 331M/331M [00:24&lt;00:00, 14.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ba72a4d5889b469a8113f5a7fd27dde7"},"model_module_version":"1.5.0"},"f7482ec2de31467dbea4753d71b8294d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"c6b0848093294970a297138cd9552e03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"},"2f6e069899f9470880ad18de95dcc539":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"},"model_module_version":"1.5.0"},"ba72a4d5889b469a8113f5a7fd27dde7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null},"model_module_version":"1.2.0"}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hbFBOE8uwJ7V","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"ok","timestamp":1590737506945,"user_tz":-540,"elapsed":9283,"user":{"displayName":"도승헌","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64","userId":"15552087445877711483"}},"outputId":"c1227db6-f689-4212-87ee-19a4ed418811"},"source":["!pip install torch\n","!pip install torchaudio"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n","Collecting torchaudio\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/7d/8e01e21175dd2c9bb1b7e014e0c56cdd02618e2db5bebb4f52f6fdf253cb/torchaudio-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 4.7MB/s \n","\u001b[?25hRequirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.5.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->torchaudio) (1.18.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->torchaudio) (0.16.0)\n","Installing collected packages: torchaudio\n","Successfully installed torchaudio-0.5.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3TqGPsmzKiUY"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"6iN54BqnSTpk"},"source":["\n","### Reference \n","\n","- Digital Signal Processing Lecture\u000bhttps://github.com/spatialaudio/digital-signal-processing-lecture \n","\n","- Python for Signal Processing (unipingco)\u000bhttps://github.com/unpingco/Python-for-Signal-Processing \n","\n","- Audio for Deep Learning (남기현님)\u000bhttps://tykimos.github.io/2019/07/04/ISS_2nd_Deep_Learning_Conference_All_Together/ \n","\n","- 오디오 전처리 작업을 위한 연습 (박수철님)\u000bhttps://github.com/scpark20/audio-preprocessing-practice \n","\n","- Musical Applications of Machine Learning\u000bhttps://mac.kaist.ac.kr/~juhan/gct634/ \n","\n","- Awesome audio study materials for Korean (최근우님)\u000bhttps://github.com/keunwoochoi/awesome-audio-study-materials-for-korean"]},{"cell_type":"code","metadata":{"id":"F4hfYNb2wu9m"},"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchaudio\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bb06mH8jxGtr","colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["4e1b1bce5c6f46fca0d63d3c5a93f9d9","c4a5238581ae454cbac49ef583e81c47","c07bf3c2e2bf4ec7a794902d3d21d882","24e332e807774ac9b476dc4dd1f714a6","178f1c9b1dfa4652ab4dc9494e2cc9af","c3f09958981c45f58d25171c0e9eab3c","53f193912acc46d0800ee862cb645e30","8b67b1196f9f4c3aaccb8c7c6bd1e3d0","26a74eca32854fc5930a5d3ad68b5e9d","afb6f5fcb4064bdb99623edb90743bd2","619ca67e4591441e91494bbf86b51216","4c7b3f5773134e4388b3aeee523b3027","f7482ec2de31467dbea4753d71b8294d","c6b0848093294970a297138cd9552e03","2f6e069899f9470880ad18de95dcc539","ba72a4d5889b469a8113f5a7fd27dde7"]},"executionInfo":{"status":"ok","timestamp":1590738275190,"user_tz":-540,"elapsed":764974,"user":{"displayName":"도승헌","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64","userId":"15552087445877711483"}},"outputId":"f4c39ba6-2915-4e99-a09b-94fcb0be8805"},"source":["train_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True) \n","test_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"test-clean\", download=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e1b1bce5c6f46fca0d63d3c5a93f9d9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=6387309499.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26a74eca32854fc5930a5d3ad68b5e9d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=346663984.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N8JvgqogxgtA","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1590738453647,"user_tz":-540,"elapsed":630,"user":{"displayName":"도승헌","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2NHRRpQvWeI7LwrrgkZX4TUzH4iggntWih2IRcQ=s64","userId":"15552087445877711483"}},"outputId":"d610f640-d56e-4e4b-abe3-eb178082af17"},"source":["test_dataset[10]\n","#소리 데이터, 샘플레이트 "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[-6.1035e-05, -1.2207e-04, -1.8311e-04,  ..., -1.0071e-03,\n","          -1.5869e-03, -1.7700e-03]]),\n"," 16000,\n"," \"BRACTON'S A VERY GOOD FELLOW I CAN ASSURE YOU\",\n"," 5683,\n"," 32866,\n"," 8)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"wlBQJ2QopRFk"},"source":["def avg_wer(wer_scores, combined_ref_len):\n","    return float(sum(wer_scores)) / float(combined_ref_len)\n","# loss 는 CTC, matrix으로 가장 대표적으로 쓰는게 리히슈타인 거리이다.\n","# 두 단어간의 차이(거리)를 보기 위해 사용하는 것 ex) ranking - corr\n","# categorical(문자)한 label이 주어졌다고 생각하기 때문에 이것을 사용한다. \n","def _levenshtein_distance(ref, hyp):\n","    \"\"\"\"Levenshtein distance\"는 두 시퀀스 간의 차이를 측정하기위한 문자열 메트릭입니다. \n","    \"Levenshtein distanc\"는 한 단어를 다른 단어로 변경하는 데 필요한 최소 한 문자 편집 (대체, 삽입 또는 삭제) 수로 정의됩니다. \n","    \"\"\"\n","    #두개의 len을 비교\n","    m = len(ref)\n","    n = len(hyp)\n","\n","    # special case\n","    if ref == hyp:\n","        return 0\n","    if m == 0:\n","        return n\n","    if n == 0:\n","        return m\n","\n","    if m < n:\n","        ref, hyp = hyp, ref\n","        m, n = n, m\n","\n","    # use O(min(m, n)) space\n","    distance = np.zeros((2, n + 1), dtype=np.int32)\n","\n","    # initialize distance matrix\n","    for j in range(0,n + 1):\n","        distance[0][j] = j\n","\n","    # calculate levenshtein distance\n","    # index 기반으로 두개의 collection을 보고\n","    # 달랐다면 두개의 차이를 맞추기 위해서는 어느정도의 연산을 해야하는지 측정\n","    for i in range(1, m + 1):\n","        prev_row_idx = (i - 1) % 2\n","        cur_row_idx = i % 2\n","        distance[cur_row_idx][0] = i\n","        for j in range(1, n + 1):\n","            if ref[i - 1] == hyp[j - 1]:\n","                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n","            else:\n","                s_num = distance[prev_row_idx][j - 1] + 1\n","                i_num = distance[cur_row_idx][j - 1] + 1\n","                d_num = distance[prev_row_idx][j] + 1\n","                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n","\n","    return distance[m % 2][n]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ru55tshPqejt"},"source":["\n","def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n","  # 단어가 틀렸는지 보는 것\n","  # white space(공백)으로 split해서 단어들이 나오면 단어가 같은지 틀린지 측정\n","    \"\"\"참조 시퀀스와 가설 시퀀스 사이의 거리를 단어 수준으로 계산합니다.\n","     : param reference : 참조 문장.\n","     : param hypothesis : 가설 문장.\n","     : param ignore_case : 대소 문자 구분 여부.\n","     : param delimiter : 입력 문장의 구분자.\n","    \"\"\"\n","    if ignore_case == True:\n","        reference = reference.lower()\n","        hypothesis = hypothesis.lower()\n","\n","    ref_words = reference.split(delimiter)\n","    hyp_words = hypothesis.split(delimiter)\n","\n","    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n","    return float(edit_distance), len(ref_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d-LCRpXVqhv8"},"source":["def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n","    # lower하게 만든 다음 두 친구가 같은지 틀린지 측정\n","    if ignore_case == True:\n","        reference = reference.lower()\n","        hypothesis = hypothesis.lower()\n","\n","    join_char = ' '\n","    if remove_space == True:\n","        join_char = ''\n","\n","    reference = join_char.join(filter(None, reference.split(' ')))\n","    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n","\n","    edit_distance = _levenshtein_distance(reference, hypothesis)\n","    return float(edit_distance), len(reference)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gBZ_3-Sqm5D"},"source":["def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n","    \"\"\"Calculate word error rate (WER). \n","    WER = (Sw + Dw + Iw) / Nw\n","    Sw는 대체 된 단어의 수입니다.\n","    Dw는 삭제 된 단어의 수입니다.\n","    Iw는 삽입 된 단어의 수입니다.\n","    Nw는 참조의 단어 수입니다.\n","    \"\"\"\n","    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n","                                         delimiter)\n","    if ref_len == 0:\n","        raise ValueError(\"Reference's word number should be greater than 0.\")\n","\n","    wer = float(edit_distance) / ref_len\n","    return wer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F6YUEk-2qqaw"},"source":["def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n","    \"\"\"Calculate charactor error rate (CER). \n","        CER = (Sc + Dc + Ic) / Nc\n","        Sc is the number of characters substituted,\n","        Dc is the number of characters deleted,\n","        Ic is the number of characters inserted\n","        Nc is the number of characters in the reference\n","    \"\"\"\n","    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n","                                         remove_space)\n","\n","    if ref_len == 0:\n","        raise ValueError(\"Length of reference should be greater than 0.\")\n","\n","    cer = float(edit_distance) / ref_len\n","    return cer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FOLv1UDe89em"},"source":["The outputs of the network are the graphemes of each language. At each output time-step t, the RNN\n","makes a prediction over characters, p(`t|x), where `t is either a character in the alphabet or the blank\n","symbol. In English we have `t ∈ {a, b, c, . . . , z,space, apostrophe, blank},"]},{"cell_type":"code","metadata":{"id":"cS2jhIrBqs_G"},"source":["class TextTransform:\n","    \"\"\"Maps characters to integers and vice versa\"\"\"\n","    def __init__(self):\n","        char_map_str = \"\"\"\n","        단어의 종류가 총 28개로 보임.\n","        ' 0\n","        <SPACE> 1\n","        a 2\n","        b 3\n","        c 4\n","        d 5\n","        e 6\n","        f 7\n","        g 8\n","        h 9\n","        i 10\n","        j 11\n","        k 12\n","        l 13\n","        m 14\n","        n 15\n","        o 16\n","        p 17\n","        q 18\n","        r 19\n","        s 20\n","        t 21\n","        u 22\n","        v 23\n","        w 24\n","        x 25\n","        y 26\n","        z 27\n","        \"\"\"\n","        self.char_map = {}\n","        self.index_map = {}\n","        for line in char_map_str.strip().split('\\n'):\n","            ch, index = line.split()\n","            self.char_map[ch] = int(index)\n","            self.index_map[int(index)] = ch\n","        self.index_map[1] = ' '\n","\n","    def text_to_int(self, text):\n","        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n","        int_sequence = []\n","        for c in text:\n","            if c == ' ':\n","                ch = self.char_map['<SPACE>']\n","            else:\n","                ch = self.char_map[c]\n","            int_sequence.append(ch)\n","        return int_sequence\n","\n","    def int_to_text(self, labels):\n","        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n","        string = []\n","        for i in labels:\n","            string.append(self.index_map[i])\n","        return ''.join(string).replace('<SPACE>', ' ')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HMUWPlL8qwQg"},"source":["train_audio_transforms = nn.Sequential(\n","    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n","    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n","    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",")\n","# torchaudio가 FrequencyMasking, TimeMasking와 같은 기능을 편하게 사용할 수 있다.\n","valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n","\n","text_transform = TextTransform()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VrbecqsFq8qM"},"source":["def data_processing(data, data_type=\"train\"):\n","    spectrograms = []\n","    labels = []\n","    input_lengths = []\n","    label_lengths = []\n","    for (waveform, _, utterance, _, _, _) in data:\n","        if data_type == 'train':\n","            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        elif data_type == 'valid':\n","            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        else:\n","            raise Exception('data_type should be train or valid')\n","        spectrograms.append(spec)\n","        # utterance가 label값인데 이를 text-to-int를 해서 int의 seqeunce로 바뀐다. \n","        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n","        labels.append(label)\n","        input_lengths.append(spec.shape[0]//2)\n","        # 아래와 같이 label을 보존하는 이유는 CTC Loss 계산에 활용해야 하기 때문\n","        label_lengths.append(len(label))\n","\n","    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n","    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n","\n","    return spectrograms, labels, input_lengths, label_lengths\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2HueYHhOMEiM"},"source":["In both cases we integrate a language model in a beam search decoding step"]},{"cell_type":"code","metadata":{"id":"RZVpzOyHq9hX"},"source":["# 아까 말했듯 논문의 마지막에 beam search를 활용하면 좋다고 했는데\n","# 구현에 있어 어려움이 있어 일단은 GreedyDecoder를 활용했음!\n","def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n","\targ_maxes = torch.argmax(output, dim=2)\n","\tdecodes = []\n","\ttargets = []\n","\tfor i, args in enumerate(arg_maxes):\n","\t\tdecode = []\n","\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n","\t\tfor j, index in enumerate(args):\n","\t\t\tif index != blank_label:\n","\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n","\t\t\t\t\tcontinue\n","\t\t\t\tdecode.append(index.item())\n","\t\tdecodes.append(text_transform.int_to_text(decode))\n","\treturn decodes, targets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQnVyQKU9PsY"},"source":["We use the clipped rectifiedlinear (ReLU) function σ(x) = min{max{x, 0}, 20} as our nonlinearity.\n","\n","The architectures we experiment\n","with consist of one or more convolutional layers, followed by one or more recurrent layers, followed\n","by one or more fully connected layers.\n","The hidden representation at layer l is given by h\n","l with the convention that h\n","0\n","represents the input\n","x. The bottom of the network is one or more convolutions over the time dimension of the input"]},{"cell_type":"code","metadata":{"id":"XXAYnQ4arD2P"},"source":["class CNNLayerNorm(nn.Module):\n","    \"\"\"Layer normalization built for cnns input\"\"\"\n","    def __init__(self, n_feats):\n","        super(CNNLayerNorm, self).__init__()\n","        self.layer_norm = nn.LayerNorm(n_feats)\n","\n","    def forward(self, x):\n","        # x (batch, channel, feature, time)\n","        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n","        x = self.layer_norm(x)\n","        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n","\n","class ResidualCNN(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n","        super(ResidualCNN, self).__init__()\n","                # 오디오이기 때문에 in_channels는 1일 것\n","        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n","        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.layer_norm1 = CNNLayerNorm(n_feats)\n","        self.layer_norm2 = CNNLayerNorm(n_feats)\n","\n","    def forward(self, x):\n","        residual = x  # (batch, channel, feature, time)\n","        x = self.layer_norm1(x)\n","        x = F.gelu(x)\n","        x = self.dropout1(x)\n","        x = self.cnn1(x)\n","        x = self.layer_norm2(x)\n","        x = F.gelu(x)\n","        x = self.dropout2(x)\n","        x = self.cnn2(x)\n","        x += residual\n","        return x # (batch, channel, feature, time)\n","# 논문에서는 Conv와 RNN 사이에 FC layer 하나를 결합했다고 나와있음\n","# 이유는 실험결과 가장 성능이 좋았기 때문이라고 한다.\n","class BidirectionalGRU(nn.Module):\n","    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n","        super(BidirectionalGRU, self).__init__()\n","        # rnn_dim과 cnn out_dim을 맞춰주는 것이 포인트!\n","        # 그런 점에서 봤을 때 FC layer를 사용하는 것도 어떻게 보면 좋은 방법일 수 있다고 함.\n","        self.BiGRU = nn.GRU(\n","            input_size=rnn_dim, hidden_size=hidden_size,\n","            num_layers=1, batch_first=batch_first, bidirectional=True)\n","        self.layer_norm = nn.LayerNorm(rnn_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.layer_norm(x)\n","        x = F.gelu(x)\n","        x, _ = self.BiGRU(x)\n","        x = self.dropout(x)\n","        return x\n","\n","class SpeechRecognitionModel(nn.Module):\n","    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n","        super(SpeechRecognitionModel, self).__init__()\n","        n_feats = n_feats//2\n","        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n","\n","        # n residual cnn layers with filter size of 32\n","        self.rescnn_layers = nn.Sequential(*[\n","            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n","            for _ in range(n_cnn_layers)\n","        ])\n","        # cnn의 n_feats와 out_dims(32)를 곱한 크기를 fc의 input_dim으로 하면 사이즈가 맞다\n","        # fc out_dim 은 rnn의 input_dim으로 사이즈를 맞춘다.\n","        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n","        self.birnn_layers = nn.Sequential(*[\n","            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n","                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n","            # rnn 을 파라미터만큼 반복해서 쌓을 수 있음\n","            for i in range(n_rnn_layers)\n","        ])\n","        self.classifier = nn.Sequential(\n","            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(rnn_dim, n_class)\n","        )\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        x = self.rescnn_layers(x)\n","        sizes = x.size()\n","        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n","        x = x.transpose(1, 2) # (batch, time, feature)\n","        x = self.fully_connected(x)\n","        x = self.birnn_layers(x)\n","        x = self.classifier(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMITaH3ALmr8"},"source":["  \n","import os\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","\n","class Dataset(Dataset):\n","    def __init__(self, x):\n","        self.x = x\n","\n","    def __getitem__(self, index):\n","        return self.x[index]\n","\n","    def __len__(self):\n","        return self.x.shape[0]\n","\n","def load_dataset(data, data_type=\"train\"):\n","    spectrograms = []\n","    labels = []\n","    input_lengths = []\n","    label_lengths = []\n","    for (waveform, _, _, _, _, _) in data:\n","        if data_type == 'train':\n","            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        else:\n","            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n","        spectrograms.append(spec)\n","        \n","    return spectrograms\n","\n","def get_dataloader(data):\n","    x_train = load_dataset(data, \"train\")\n","    x_valid = load_dataset(data, 'valid')\n","\n","    mean = np.mean(x_train)\n","    std = np.std(x_train)\n","    x_train = (x_train - mean)/std\n","    x_valid = (x_valid - mean)/std\n","\n","    train_set = Dataset(x_train)\n","    vaild_set = Dataset(x_valid)\n","\n","    train_loader = DataLoader(train_set, batch_size=4, shuffle=True, drop_last=False)\n","    valid_loader = DataLoader(vaild_set, batch_size=4, shuffle=False, drop_last=False)\n","    \n","    return train_loader, valid_loader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSRFH83jetB_"},"source":["이 튜토리얼에서는 \"greedy\"디코딩 방법을 사용하여 모델의 출력을 문자로 결합하여 대화 내용을 만들 수 있습니다. \"greedy\"디코더는 모델의 소프트 맥스 확률 문자 인 모델 출력을 취하며 각 시간 단계에 대해 가장 높은 확률을 가진 레이블을 선택합니다. 라벨이 빈 라벨 인 경우 최종 성적표에서 라벨이 제거됩니다."]},{"cell_type":"code","metadata":{"id":"Omm1e-e4Mu0V"},"source":["class IterMeter(object):\n","    \"\"\"keeps track of total iterations\"\"\"\n","    def __init__(self):\n","        self.val = 0\n","\n","    def step(self):\n","        self.val += 1\n","\n","    def get(self):\n","        return self.val\n","\n","\n","def train(model, device, train_loader, criterion, optimizer, epoch, iter_meter):\n","    model.train()\n","    data_len = len(train_loader.dataset)\n","    for batch_idx, _data in enumerate(train_loader):\n","        spectrograms, labels, input_lengths, label_lengths = _data \n","        spectrograms, labels = spectrograms.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        output = model(spectrograms)  # (batch, time, n_class)\n","        output = F.log_softmax(output, dim=2)\n","        output = output.transpose(0, 1) # (time, batch, n_class)\n","\n","        loss = criterion(output, labels, input_lengths, label_lengths)\n","        loss.backward()\n","\n","        optimizer.step()\n","        iter_meter.step()\n","        if batch_idx % 100 == 0 or batch_idx == data_len:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(spectrograms), data_len,\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n","\n","def test(model, device, test_loader, criterion, epoch, iter_meter):\n","    print('\\nevaluating...')\n","    model.eval()\n","    test_loss = 0\n","    test_cer, test_wer = [], []\n","    with torch.no_grad():\n","        for i, _data in enumerate(test_loader):\n","            spectrograms, labels, input_lengths, label_lengths = _data \n","            spectrograms, labels = spectrograms.to(device), labels.to(device)\n","\n","            output = model(spectrograms)  # (batch, time, n_class)\n","            # The output layer L is a softmax computing a probability distribution over characters given\n","            output = F.log_softmax(output, dim=2)\n","            output = output.transpose(0, 1) # (time, batch, n_class)\n","\n","            loss = criterion(output, labels, input_lengths, label_lengths)\n","            test_loss += loss.item() / len(test_loader)\n","\n","            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n","            for j in range(len(decoded_preds)):\n","                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n","                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n","\n","\n","    avg_cer = sum(test_cer)/len(test_cer)\n","    avg_wer = sum(test_wer)/len(test_wer)\n","    \n","    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n","\n","\n","def main(learning_rate=5e-4, batch_size=20, epochs=10,\n","        train_url=\"train-clean-100\", test_url=\"test-clean\"):\n","\n","    hparams = {\n","        \"n_cnn_layers\": 3,\n","        \"n_rnn_layers\": 5,\n","        \"rnn_dim\": 512,\n","        \"n_class\": 29, # 총 28종류 + epsilon = 29\n","        \"n_feats\": 128,\n","        \"stride\":2,\n","        \"dropout\": 0.1,\n","        \"learning_rate\": learning_rate,\n","        \"batch_size\": batch_size,\n","        \"epochs\": epochs\n","    }\n","\n","\n","    use_cuda = torch.cuda.is_available()\n","    torch.manual_seed(7)\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if not os.path.isdir(\"./data\"):\n","        os.makedirs(\"./data\")\n","\n","    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n","    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n","\n","    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","    train_loader = data.DataLoader(dataset=train_dataset,\n","                                batch_size=hparams['batch_size'],\n","                                shuffle=True,\n","                                collate_fn=lambda x: data_processing(x, 'train'),\n","                                **kwargs)\n","    test_loader = data.DataLoader(dataset=test_dataset,\n","                                batch_size=hparams['batch_size'],\n","                                shuffle=False,\n","                                collate_fn=lambda x: data_processing(x, 'valid'),\n","                                **kwargs)\n","\n","    model = SpeechRecognitionModel(\n","        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n","        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n","        ).to(device)\n","\n","    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n","\n","    optimizer = optim.Adam(model.parameters(), hparams['learning_rate'])\n","    criterion = nn.CTCLoss(blank=28).to(device)\n","\n","    iter_meter = IterMeter()\n","    for epoch in range(1, epochs + 1):\n","        train(model, device, train_loader, criterion, optimizer, epoch, iter_meter)\n","        test(model, device, test_loader, criterion, epoch, iter_meter)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HxNEtE7yP-DA","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"2b3fe81c-ac9b-45e0-fad5-fed60b8faf96"},"source":["learning_rate = 5e-4\n","batch_size = 10\n","epochs = 1\n","libri_train_set = \"train-clean-100\"\n","libri_test_set = \"test-clean\"\n","\n","main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Num Model Parameters 23705373\n","Train Epoch: 1 [0/28539 (0%)]\tLoss: 7.220805\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C0wPUMERrnjX"},"source":[""],"execution_count":null,"outputs":[]}]}